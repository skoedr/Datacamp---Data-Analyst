{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"..\\\\12_Merging DataFrames with pandas\\\\data\\\\\"\n",
    "\n",
    "feb_sales_hardware_path = path + 'Sales\\\\' + 'feb-sales-Hardware.csv'\n",
    "feb_sales_service_path = path + 'Sales\\\\' + 'feb-sales-Service.csv'\n",
    "feb_sales_software_path = path + 'Sales\\\\' + 'feb-sales-Sofware.csv'\n",
    "sales_feb_2015 = path + 'Sales\\\\' + 'sales-feb-2015.csv'\n",
    "sales_jan_2015 = path + 'Sales\\\\' + 'sales-jan-2015.csv'\n",
    "sales_mar_2015 = path + 'Sales\\\\' + 'sales-mar-2015.csv'\n",
    "names1881_path = path + '\\\\Baby names\\\\' + 'names1881.csv'\n",
    "names1981_path = path + '\\\\Baby names\\\\' + 'names1981.csv'\n",
    "medals_path = path + 'Summer Olympic medals\\\\'\n",
    "bronze_top5_path = path + 'Summer Olympic medals\\\\' + 'bronze_top5.csv'\n",
    "silver_top5_path = path + 'Summer Olympic medals\\\\' + 'silver_top5.csv'\n",
    "gold_top5_path = path + 'Summer Olympic medals\\\\' + 'gold_top5.csv'\n",
    "gdp_usa = path + 'GDP\\\\' + 'gdp_usa.csv'\n",
    "gdp_china = path + 'GDP\\\\' + 'gdp_china.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending & concatenating Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending Series with nonunique Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Series bronze and silver, which have been printed in the IPython Shell, represent the 5 countries that won the most bronze and silver Olympic medals respectively between 1896 & 2008. The Indexes of both Series are called Country and the values are the corresponding number of medals won.\n",
    "\n",
    "If you were to run the command combined = bronze.append(silver), how many rows would combined have? And how many rows would combined.loc['United States'] return? Find out for yourself by running these commands in the IPython Shell.\n",
    "\n",
    "- combined has 5 rows and combined.loc['United States'] is empty (0 rows).\n",
    " - combined has 10 rows and combined.loc['United States'] has 2 rows.\n",
    "- combined has 6 rows and combined.loc['United States'] has 1 row.\n",
    "- combined has 5 rows and combined.loc['United States'] has 2 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bronze\n",
    "# Country\n",
    "# United States     1052.0\n",
    "# Soviet Union       584.0\n",
    "# United Kingdom     505.0\n",
    "# France             475.0\n",
    "# Germany            454.0\n",
    "# Name: Total, dtype: float64\n",
    "\n",
    "# silver\n",
    "# Country\n",
    "# United States     1195.0\n",
    "# Soviet Union       627.0\n",
    "# United Kingdom     591.0\n",
    "# France             461.0\n",
    "# Italy              394.0\n",
    "# Name: Total, dtype: float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending pandas Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll load sales data from the months January, February, and March into DataFrames. Then, you'll extract Series with the 'Units' column from each and append them together with method chaining using .append().\n",
    "\n",
    "To check that the stacking worked, you'll print slices from these Series, and finally, you'll add the result to figure out the total units sold in the first quarter.\n",
    "\n",
    "- Read the files 'sales-jan-2015.csv', 'sales-feb-2015.csv' and 'sales-mar-2015.csv' into the DataFrames jan, feb, and mar respectively.\n",
    "- Use parse_dates=True and index_col='Date'.\n",
    "- Extract the 'Units' column of jan, feb, and mar to create the Series jan_units, feb_units, and mar_units respectively.\n",
    "- Construct the Series quarter1 by appending feb_units to jan_units and then appending mar_units to the result. Use chained calls to the .append() method to do this.\n",
    "- Verify that quarter1 has the individual Series stacked vertically. To do this:\n",
    "- Print the slice containing rows from jan 27, 2015 to feb 2, 2015.\n",
    "- Print the slice containing rows from feb 26, 2015 to mar 7, 2015.\n",
    "- Compute and print the total number of units sold from the Series quarter1. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64 \n",
      "\n",
      "Date\n",
      "2015-02-26 08:57:45     4\n",
      "2015-02-26 08:58:51     1\n",
      "2015-03-06 10:11:45    17\n",
      "2015-03-06 02:03:56    17\n",
      "Name: Units, dtype: int64 \n",
      "\n",
      "642\n"
     ]
    }
   ],
   "source": [
    "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
    "jan = pd.read_csv(sales_jan_2015, parse_dates=True, index_col='Date')\n",
    "\n",
    "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
    "feb = pd.read_csv(sales_feb_2015, parse_dates=True, index_col='Date')\n",
    "\n",
    "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
    "mar = pd.read_csv(sales_mar_2015, parse_dates=True, index_col='Date')\n",
    "\n",
    "# Extract the 'Units' column from jan: jan_units\n",
    "jan_units = jan['Units']\n",
    "\n",
    "# Extract the 'Units' column from feb: feb_units\n",
    "feb_units = feb['Units']\n",
    "\n",
    "# Extract the 'Units' column from mar: mar_units\n",
    "mar_units = mar['Units']\n",
    "\n",
    "# Append feb_units and then mar_units to jan_units: quarter1\n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
    "\n",
    "# Print the first slice from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'], '\\n')\n",
    "\n",
    "# Print the second slice from quarter1\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'], '\\n')\n",
    "\n",
    "# Compute & print total sales in quarter1\n",
    "print(quarter1.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating pandas Series along row axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having learned how to append Series, you'll now learn how to achieve the same result by concatenating Series instead. You'll continue to work with the sales data you've seen previously. This time, the DataFrames jan, feb, and mar have been pre-loaded.\n",
    "\n",
    "Your job is to use pd.concat() with a list of Series to achieve the same result that you would get by chaining calls to .append().\n",
    "\n",
    "You may be wondering about the difference between pd.concat() and pandas' .append() method. One way to think of the difference is that .append() is a specific case of a concatenation, while pd.concat() gives you more flexibility, as you'll see in later exercises.\n",
    "\n",
    "- Create an empty list called units. This has been done for you.\n",
    "- Use a for loop to iterate over [jan, feb, mar]:\n",
    "- In each iteration of the loop, append the 'Units' column of each DataFrame to units.\n",
    "- Concatenate the Series contained in the list units into a longer Series called quarter1 using pd.concat().\n",
    "- Specify the keyword argument axis='rows' to stack the Series vertically.\n",
    "- Verify that quarter1 has the individual Series stacked vertically by printing slices. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64 \n",
      "\n",
      "Date\n",
      "2015-02-26 08:57:45     4\n",
      "2015-02-26 08:58:51     1\n",
      "2015-03-06 10:11:45    17\n",
      "2015-03-06 02:03:56    17\n",
      "Name: Units, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty list: units\n",
    "units = []\n",
    "\n",
    "# Build the list of Series\n",
    "for month in [jan, feb, mar]:\n",
    "    units.append(month['Units'])\n",
    "\n",
    "# Concatenate the list: quarter1\n",
    "quarter1 = pd.concat(units)\n",
    "\n",
    "# Print slices from quarter1\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'], '\\n')\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending & concatenating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending DataFrames with ignore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll use the Baby Names Dataset (from data.gov) again. This time, both DataFrames names_1981 and names_1881 are loaded without specifying an Index column (so the default Indexes for both are RangeIndexes).\n",
    "\n",
    "You'll use the DataFrame .append() method to make a DataFrame combined_names. To distinguish rows from the original two DataFrames, you'll add a 'year' column to each with the year (1881 or 1981 in this case). In addition, you'll specify ignore_index=True so that the index values are not used along the concatenation axis. The resulting axis will instead be labeled 0, 1, ..., n-1, which is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information.\n",
    "\n",
    "- Create a 'year' column in the DataFrames names_1881 and names_1981, with values of 1881 and 1981 respectively. Recall that assigning a scalar value to a DataFrame column broadcasts that value throughout.\n",
    "- Create a new DataFrame called combined_names by appending the rows of names_1981 underneath the rows of names_1881. Specify the keyword argument ignore_index=True to make a new RangeIndex of unique integers for each row.\n",
    "- Print the shapes of all three DataFrames. This has been done for you.\n",
    "- Extract all rows from combined_names that have the name 'Morgan'. To do this, use the .loc[] accessor with an appropriate filter. The relevant column of combined_names here is 'name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1935, 4)\n",
      "(1935, 4)\n",
      "(3870, 4)\n",
      "        name gender  count  year\n",
      "1283  Morgan      M     23  1881\n",
      "3218  Morgan      M     23  1981\n"
     ]
    }
   ],
   "source": [
    "# preparations\n",
    "names_1881 = pd.read_csv(names1881_path, header=None, names=['name','gender','count'])\n",
    "names_1981 = pd.read_csv(names1881_path, header=None, names=['name','gender','count'])\n",
    "\n",
    "# Add 'year' column to names_1881 and names_1981\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981\n",
    "\n",
    "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
    "combined_names = names_1881.append(names_1981, ignore_index=True)\n",
    "\n",
    "# Print shapes of names_1981, names_1881, and combined_names\n",
    "print(names_1981.shape)\n",
    "print(names_1881.shape)\n",
    "print(combined_names.shape)\n",
    "\n",
    "# Print all rows that contain the name 'Morgan'\n",
    "print(combined_names.loc[combined_names['name']=='Morgan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating pandas DataFrames along column axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function pd.concat() can concatenate DataFrames horizontally as well as vertically (vertical is the default). To make the DataFrames stack horizontally, you have to specify the keyword argument axis=1 or axis='columns'.\n",
    "\n",
    "In this exercise, you'll use weather data with maximum and mean daily temperatures sampled at different rates (quarterly versus monthly). You'll concatenate the rows of both and see that, where rows are missing in the coarser DataFrame, null values are inserted in the concatenated DataFrame. This corresponds to an outer join (which you will explore in more detail in later exercises).\n",
    "\n",
    "The files 'quarterly_max_temp.csv' and 'monthly_mean_temp.csv' have been pre-loaded into the DataFrames weather_max and weather_mean respectively, and pandas has been imported as pd.\n",
    "\n",
    "- Create weather_list, a list of the DataFrames weather_max and weather_mean.\n",
    "- Create a new DataFrame called weather by concatenating weather_list horizontally.\n",
    "- Pass the list to pd.concat() and specify the keyword argument axis=1 to stack them horizontally.\n",
    "- Print the new DataFrame weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Max TemperatureF  Mean TemperatureF\n",
      "Jan              68.0          32.354839\n",
      "Apr              89.0          53.100000\n",
      "Jul              91.0          72.870968\n",
      "Oct              84.0          55.451613\n",
      "Aug               NaN          70.000000\n",
      "Dec               NaN          34.935484\n",
      "Feb               NaN          28.714286\n",
      "Jun               NaN          70.133333\n",
      "Mar               NaN          35.000000\n",
      "May               NaN          62.612903\n",
      "Nov               NaN          39.800000\n",
      "Sep               NaN          63.766667\n"
     ]
    }
   ],
   "source": [
    "weather_max = pd.DataFrame({'month':['Jan', 'Apr', 'Jul', 'Oct'],\n",
    "                      'Max TemperatureF':[68, 89, 91, 84]}).set_index('month')\n",
    "weather_mean = pd.DataFrame({'month':['Apr', 'Aug', 'Dec', 'Feb', 'Jan', 'Jul', 'Jun', 'Mar', 'May', 'Nov', 'Oct', 'Sep'],\n",
    "                      'Mean TemperatureF':[53.1, 70.0, 34.93548387, 28.71428571, 32.35483871, 72.87096774, 70.13333333,\n",
    "                                           35.0, 62.61290323, 39.8, 55.4516129, 63.76666667]}).set_index('month')\n",
    "\n",
    "# Create a list of weather_max and weather_mean\n",
    "weather_list = [weather_max, weather_mean]\n",
    "\n",
    "# Concatenate weather_list horizontally\n",
    "weather = pd.concat(weather_list, axis=1)\n",
    "\n",
    "# Print weather\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading multiple files to build a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often convenient to build a large DataFrame by parsing many files as DataFrames and concatenating them all at once. You'll do this here with three files, but, in principle, this approach can be used to combine data from dozens or hundreds of files.\n",
    "\n",
    "Here, you'll work with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "pandas has been imported as pd and the list medal_types has been pre-loaded for you, which contains the strings 'bronze', 'silver', and 'gold'.\n",
    "\n",
    "- Iterate over medal_types in the for loop.\n",
    "- Inside the for loop:\n",
    " - Create file_name using string interpolation with the loop variable medal. This has been done for you. The expression \"%s_top5.csv\" % medal evaluates as a string with the value of medal replacing %s in the format string.\n",
    " - Create the list of column names called columns. This has been done for you.\n",
    " - Read file_name into a DataFrame called medal_df. Specify the keyword arguments header=0, index_col='Country', and names=columns to get the correct row and column Indexes.\n",
    " - Append medal_df to medals using the list .append() method.\n",
    "- Concatenate the list of DataFrames medals horizontally (using axis='columns') to create a single DataFrame called medals_df. Print it in its entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                bronze  silver    gold\n",
      "United States   1052.0  1195.0  2088.0\n",
      "Soviet Union     584.0   627.0   838.0\n",
      "United Kingdom   505.0   591.0   498.0\n",
      "France           475.0   461.0     NaN\n",
      "Germany          454.0     NaN   407.0\n",
      "Italy              NaN   394.0   460.0\n"
     ]
    }
   ],
   "source": [
    "medal_types = ['bronze', 'silver', 'gold']\n",
    "\n",
    "#Initialize an empyy list: medals\n",
    "medals =[]\n",
    "\n",
    "for medal in medal_types:\n",
    "    # Create the file name: file_name\n",
    "    file_name = medals_path + \"%s_top5.csv\" % medal\n",
    "    # Create list of column names: columns\n",
    "    columns = ['Country', medal]\n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name, header=0, index_col='Country', names=columns)\n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "\n",
    "# Concatenate medals horizontally: medals_df\n",
    "medals_df = pd.concat(medals, axis='columns')\n",
    "\n",
    "# Print medals_df\n",
    "print(medals_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation, keys, & MultiIndexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating DataFrames with inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "The DataFrames bronze, silver, and gold have been pre-loaded for you.\n",
    "\n",
    "Your task is to compute an inner join.\n",
    "\n",
    "- Construct a list of DataFrames called medal_list with entries bronze, silver, and gold.\n",
    "- Concatenate medal_list horizontally with an inner join to create medals.\n",
    "- Use the keyword argument keys=['bronze', 'silver', 'gold'] to yield suitable hierarchical indexing.\n",
    "- Use axis=1 to get horizontal concatenation.\n",
    "- Use join='inner' to keep only rows that share common index labels.\n",
    "- Print the new DataFrame medals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                bronze  silver    gold\n",
      "                 Total   Total   Total\n",
      "Country                               \n",
      "United States   1052.0  1195.0  2088.0\n",
      "Soviet Union     584.0   627.0   838.0\n",
      "United Kingdom   505.0   591.0   498.0\n"
     ]
    }
   ],
   "source": [
    "bronze = pd.read_csv(bronze_top5_path, index_col='Country')\n",
    "silver = pd.read_csv(silver_top5_path, index_col='Country')\n",
    "gold = pd.read_csv(gold_top5_path, index_col='Country')\n",
    "\n",
    "# Create the list of DataFrames: medal_list\n",
    "medal_list = [bronze, silver, gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list, keys=['bronze', 'silver', 'gold'], axis=1, join='inner')\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling & concatenating DataFrames with inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll compare the historical 10-year GDP (Gross Domestic Product) growth in the US and in China. The data for the US starts in 1947 and is recorded quarterly; by contrast, the data for China starts in 1961 and is recorded annually.\n",
    "\n",
    "You'll need to use a combination of resampling and an inner join to align the index labels. You'll need an appropriate offset alias for resampling, and the method .resample() must be chained with some kind of aggregation method (.pct_change() and .last() in this case).\n",
    "\n",
    "pandas has been imported as pd, and the DataFrames china and us have been pre-loaded, with the output of china.head() and us.head() printed in the IPython Shell.\n",
    "\n",
    "- Make a new DataFrame china_annual by resampling the DataFrame china with .resample('A').last() (i.e., with annual frequency) and chaining two method calls:\n",
    "- Chain .pct_change(10) as an aggregation method to compute the percentage change with an offset of ten years.\n",
    "- Chain .dropna() to eliminate rows containing null values.\n",
    "- Make a new DataFrame us_annual by resampling the DataFrame us exactly as you resampled china.\n",
    "- Concatenate china_annual and us_annual to construct a DataFrame called gdp. Use join='inner' to perform an inner join and use axis=1 to concatenate horizontally.\n",
    "- Print the result of resampling gdp every decade (i.e., using .resample('10A')) and aggregating with the method .last(). This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               China        US\n",
      "Year                          \n",
      "1970-12-31  0.546128  1.017187\n",
      "1980-12-31  1.072537  1.742556\n",
      "1990-12-31  0.892820  1.012126\n",
      "2000-12-31  2.357522  0.738632\n",
      "2010-12-31  4.011081  0.454332\n",
      "2020-12-31  3.789936  0.361780\n"
     ]
    }
   ],
   "source": [
    "us = pd.read_csv(gdp_usa, index_col='Year', parse_dates=True, names=['Year','US'], header=None, skiprows=1)\n",
    "china = pd.read_csv(gdp_china, index_col='Year', parse_dates=True, names=['Year','China'], header=None, skiprows=1)\n",
    "\n",
    "\n",
    "# Resample and tidy china: china_annual\n",
    "china_annual = china.resample('A').last().pct_change(10).dropna()\n",
    "\n",
    "# Resample and tidy us: us_annual\n",
    "us_annual = us.resample('A').last().pct_change(10).dropna()\n",
    "\n",
    "# Concatenate china_annual and us_annual: gdp\n",
    "gdp = pd.concat([china_annual, us_annual], join='inner', axis=1)\n",
    "\n",
    "# Resample gdp and print\n",
    "print(gdp.resample('10A').last())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
